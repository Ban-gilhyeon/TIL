스키마 레지스트리와 프로듀서, 컨슈머를 이용해 메시지를 주고 받으며, 카프카 커넥트를 이용해 2개의 카프카 클러스터를 미러링하는 동작 구성 및 모니터링 시스템 구성 
## 앤터프라이즈용 카프카 아키텍처의 개요 
- 앤터프라이즈 환경에서 장애 복구 등을 위해 하나 이상의 데이터 센터 운영 
- 카프카 클러스터 역시 다수의 데이터 센터에 배치되어 있으므로, 카프카와 카프카 간의 데이터 `리플리케이션`은 필수
- 리플리케이션에는 주로 미러 메이커를 사용 <br>
![[Pasted image 20250222191214.png]]
1. 업스트림 카프카와 다운스트림 카프카 사이에 리플레케이션이 필요한 경우
2. 데이터 센터마다 카프카가 존재하고 중앙에 있는 카프카로 데이터를 모으기 위해 리플리케이션을 사용하기도 합니다. 
	- 장기 계획 배치가 가능한 하둡
	- 실시간 검색이 가능한 엘라스틱 서치
	- AWS의 대표적인 스토리지인 s3
	- 빅데이터를 실시간으로 읽고 쓸 수 있는 HBase
![[Pasted image 20250222193748.png]]
- 카프카의 토픽을 컨슘해야 하므로, 각 애플리케이션 마다 각기 다른 컨슈머가 필요
- 각 애플리케이션에 적재가 완료되면 장기 계획 배치 작업, 실시간 검색 및 여러 가지 데이터 분석 작업 가능

![[Pasted image 20250223000306.png]]

- 카프카 클러스터 총 2개 업스트림 클러스터와 다운스트림 카프카
- 다운 스트림 카프카는 업스트림 카프카를 미러링 
	- 카프카 간의 미러링을 위해 카프카 커넥트 기반의 미러 메이커 2.0 동작
- 양쪽의 카프카는 효율적인 스키마 관리를 위해 스키마 레지스트리를 사용

데이터 흐름
- 프로듀서-1이 업스트림 카프카로 메세지 전송 (스키마 레지스트리 사용)
	- 미러 메이커 2.0을 통해 다운스트림 카프카로 미러링
	- 실시간 분석을 위해 컨슈머-2가 다운스트림 카프카로부터 메세지를 읽은 후 엘라스틱서치로 메세지 전송
	- 메세지는 최종적으로 엘라스틱서치에 모두 저장 
	- 엘라스틱 서치에서 제공하는 REST API 또는 키바나를 통해 데이터를 확인
- 컨슈머-1은 업스트림카프카로부터 메시지를 읽음 (스키마 레지스트리 사용)

모니터링 측면
- 업스트림 카프카와 다운스트림 카프카는 `CMAK(구 카프카 매니저)`를 이용해 카프카의 토픽 관리
- 양쪽 카프카에서 발생하는 메트릭을 프로메테우스에 저장
- 그라파나를 통해 그래프 형태로 모니터링

## 엔터프라이즈용 카프카의 환경 구성

![[Pasted image 20250223001858.png]]
- kafka01~kafka03 업스트링 카프카
- zk01~zk03 다운스트림 카프카
- 주키퍼는 2개의 앙상블을 구성하지 않고 1개의 앙상블로 구성
- 지노드(zNode)를 분리해 카프카 클러스터 2개가 하나의 주키퍼 앙상블을 바라보도록 구성

*카프카와 카프카 가느이 미러링에서 가장 중요한 것은 메세지 손실이 없어야 함*
미러링 동작
1. 업스트림 카프카의 토픽에서 메세지 읽기
2. 업스트림 카프카에서 읽은 메세지를 다운스트림 카프카의 토픽으로 전송
즉, 카프카에서 미러링 동작은 프로듀서 + 컨슈머 동작으로 이뤄짐

카프카 커넥트의 위치 
업스트림 카프카는 부산, 다운스트림 카프카는 서울에 있다면 
- 미러링을 위한 카프카 커넥트가 업스트림(부산)에 있다면
	- 업스트림 카프카의 토픽을 컨슘하는 컨슈머와 다운스트림 카프카의 토픽을 전송하는 프로듀서가 있음 
	- 업스트림 카프카에서 `컨슘하는 동작`은 로컬네트워크에서 이뤄짐(네트워크 장애 낮음)
	- 업스트림 카프카의 `토픽으로 전송`하는 동작은 네트워크 전용선 또는 인터넷 구간에서 이뤄짐 (네트워크 회선 장애등의 상황에 노출될 가능성이 높음/ 메세지 유실 가능성)
- 미러링을 위한 카프카 커넥트가 다운스트림(서울)에 있다면
	- 업스트림 카프카에서 `컨슘하는 동작`이 전용선 또는 인터넷 구간에서 이뤄짐
		- 전용선 또는 네트워크 구간에서 회선 장애등의 문제가 발생할 경우 
		- 컨슈머의 오프셋을 장애 전 시점으로만 변경한다면 일부 메세지의 중복 발생 / 메세지 유실은 되지 않음
	- 다운스트림 카프카의 `토픽으로 전송하는` 로컬 네트워크에서 이뤄짐 
	<br>
		프로듀서는 컨슈머보다 네트워크 장애에 더욱 민감하므로 다운스트림 카프카와 가까운 위치에 배치하는 것이 중요

## 엔터프라이즈용 카프카의 운영 실습
엔서블을 이용해 전체 애플리케이션 설치 
```
> cd kafka2/  
> cd chapter12/ansible_playbook 
> ansible-playbook -i hosts site.yml
> ansible-playbook -i hosts expoter.yml 
> ansible-playbook -i hosts monitoring.yml
```

### CMAK를 이용한 토픽 생성
`http://peter-kafka01.foo.bar:9000` 주소로 접근 
![[Pasted image 20250223015021.png]]
- Add Cluster 
	- Cluster Name 입력
	- Cluster Zookeeper host에 사진과 같이 입력
	- Enable JMX Polling 체크박스 활성화 
	- Save

![[Pasted image 20250223015803.png]]

### 카프카 커넥트 설정
 ```
 > curl http://peter-zk01.foo.bar:8083/connectors (1)
 > curl --header "Content-Type: application/json" --header "Accept: application/json" --request  PUT --data '{
 "name": "kisu-mirrormaker2","connector.class": "org.apache.kafka.connect.mirror.MirrorSourceConnector",
 "tasks.max": "1",
 "source.cluster.alias": "src", (2)
 "target.cluster.alias": "dst", (2)
 "source.cluster.bootstrap.servers": "kisu-kafka01.foo.bar:9092, (3)
 kisu-kafka02.foo.bar:9092,kisu-kafka03.foo.bar:9092",
 "target.cluster.bootstrap.servers": "kisu-zk01.foo.bar:9092,kisu-zk02.foo.bar:9092,kisu-zk03.foo.bar:9092",
 "replication.factor": "3",(4)
 "topics": "kisu-avro01-kafka1" 
 }' http://peter-zk01.foo.bar:8083/connectors/kisu-mirrormaker2/config 
 > curl http:/peter-zk01.foo.bar:8083/connectors/kisu-mirrormaker2/status | python -m json.tool
```
  1. 커넥터에서 사용하는 클래스 정의 
  2. 소스 클러스터와 타깃 클러스터의 에일리어스 지정
  3. 소스 클러스터와 타깃 클러스터의 부트스트랩 서버 리스트 설정
  4. 타깃 클러스터의 토픽을 생성할 때 리플리케이션 팩터 수 지정

카프카 커넥트에 잘 등록 되었는지 다음 명령어로 확인
```
> curl http://peter-zk01.foo.bar:8083/connectors/peter-mirrormaker2/status | python -m json.tool
```

![[Pasted image 20250223023604.png]]
CMAK 메인화면에서 Cluster > List > kafka-2 > Topic > List 로 이동하면 확인 가능
- 분산 모드 카프카 커넥트 실행 -> 커넥트에서 사용하는 토픽들 생성 `src.peter-avro01-kafka1` 확인
- `src.peter-avro01-kafka1` 토픽이 미러 메이커를 이용해 업스트림 카프카의 `peter-avro01-kafka1` 토픽 미러링

### 모니터링 환경 구성
- 그라파나와 프로메테우스는 `peter-kafka01` 서버에 설치되어 있음
- `http://peter-kafka-01.foo.bar:3000`로 그라파나에 접근 
- 초기 아이디와 비밀번호는 `admin`

프로메테우스와 그라파나 연동
- 그라파나 대시보드 생성 절의 내용과 동일
- Name : Prometheus
- URL : `http://peter-kafka01.foo.bar:9090`
- save & test 
- kafka metrics 대시보드와 kafka Exporter Overview 대시보드 작성
- Node Exporter 대시보드 구성
![[Pasted image 20250223174946.png]]

### 메세지 전송과 확인(p.408 참고)

파이썬 클라이언트를 직접 구성하고 해당 클라이언트들과 스키마 레지스트리를 이용해 메세지 발신 수신 
